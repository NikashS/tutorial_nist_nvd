{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Inputs for Network Visualization in D3.js and Gephi\n",
    "## Using NIST National Vulnerabilities Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Necessary Packages into Python for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the necessary packages to process the NIST National Vulnerabilities Database (NVD) for 2015. The xml file is available here: [NIST NVD Website](https://nvd.nist.gov/download.cfm#CVE_FEED). The packages used are:\n",
    "    1. re for parsing the xml with regex\n",
    "    2. csv for exporting to comma separated variable file\n",
    "    3. json for exporting to JSON\n",
    "    4. datetime for determining week number of year and day number of week from timestamp\n",
    "    5. (Optional) tqdm for creating a cheap progress bar on an iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, csv, json, datetime\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function to extract necessary information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a function that can parse the NIST NVD xml file and extract the relevant information. We parse out more fields than is needed for our visualisations. The additional information can be used in the future to create more insights into the NIST NVD. The function returns a dictionary by vulnerability key as defined by https://cve.mitre.org/. All additional information is then stored in a dictionary for that particular vulnerability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_nvd(fname):\n",
    "    print 'Loading NVD Dataset: %s'%fname\n",
    "    nvd_dict = {}\n",
    "    with open(fname) as f:\n",
    "        for ln in tqdm(f):\n",
    "            if 'entry id=' in ln:\n",
    "                vuln_id = re.search('CVE.\\d\\d\\d\\d.\\d\\d\\d\\d', ln).group(0)\n",
    "                # print '************************%s************************'%vuln_id\n",
    "                nvd_dict[vuln_id] = {}\n",
    "            elif 'cpe-lang:fact-ref name=' in ln:\n",
    "                nvd_dict[vuln_id]['vuln_os'] = []\n",
    "                nvd_dict[vuln_id]['vuln_os_plat'] = []\n",
    "                vuln_os = re.search('\\\".*\\\"', ln).group(0)[8:-1]\n",
    "                vuln_os_plat = re.split('[:\\\"]', ln)\n",
    "                # print '--->%s'%vuln_os\n",
    "                nvd_dict[vuln_id]['vuln_os'].append(re.sub('_',' ',vuln_os_plat[4]))\n",
    "                nvd_dict[vuln_id]['vuln_os_plat'].append(re.sub('_',' ',vuln_os_plat[4])+' '+re.sub('_',' ',vuln_os_plat[5]))\n",
    "            elif '<vuln:published-datetime>' in ln:\n",
    "                vuln_date = re.search('\\d\\d\\d\\d.*<', ln).group(0)[:-1]\n",
    "                date = [int(vuln_date[0:4]),int(vuln_date[5:7]),int(vuln_date[8:10]), int(vuln_date[11:13]), int(vuln_date[14:16]), int(vuln_date[17:19])]\n",
    "                date_num = datetime.date(date[0],date[1],date[2]).isocalendar()\n",
    "                datetime_num = datetime.datetime(date[0],date[1],date[2],date[3],date[4],date[5]).strftime('%Y%m%d %X')\n",
    "                # print '    --->%s'%vuln_date\n",
    "                nvd_dict[vuln_id]['vuln_date'] = date_num\n",
    "                nvd_dict[vuln_id]['vuln_datetime'] = datetime_num\n",
    "            elif '<cvss:score>' in ln:\n",
    "                vuln_score = re.search('\\d?\\d.\\d', ln).group(0)\n",
    "                # print '    --->%s'%vuln_score\n",
    "                nvd_dict[vuln_id]['vuln_score'] = vuln_score\n",
    "            elif '<cvss:access-vector>' in ln:\n",
    "                vuln_vector = re.search('>.*<', ln).group(0)[1:-1]\n",
    "                # print '    --->%s'%vuln_vector\n",
    "                nvd_dict[vuln_id]['vuln_vector'] = vuln_vector\n",
    "            elif '<cvss:access-complexity>' in ln:\n",
    "                vuln_compl = re.search('>.*<', ln).group(0)[1:-1]\n",
    "                # print '    --->%s'%vuln_compl\n",
    "                nvd_dict[vuln_id]['vuln_compl'] = vuln_compl\n",
    "            elif '<cvss:authentication>' in ln:\n",
    "                vuln_auth = re.search('>.*<', ln).group(0)[1:-1]\n",
    "                # print '    --->%s'%vuln_auth\n",
    "                nvd_dict[vuln_id]['vuln_auth'] = vuln_auth\n",
    "            elif '<cvss:confidentiality-impact>' in ln:\n",
    "                vuln_confid = re.search('>.*<', ln).group(0)[1:-1]\n",
    "                # print '    --->%s'%vuln_confid\n",
    "                nvd_dict[vuln_id]['vuln_confid'] = vuln_confid\n",
    "            elif '<cvss:integrity-impact>' in ln:\n",
    "                vuln_integ = re.search('>.*<', ln).group(0)[1:-1]\n",
    "                # print '    --->%s'%vuln_integ\n",
    "                nvd_dict[vuln_id]['vuln_integ'] = vuln_integ\n",
    "            elif '<cvss:availability-impact>' in ln:\n",
    "                vuln_avail = re.search('>.*<', ln).group(0)[1:-1]\n",
    "                # print '    --->%s'%vuln_avail\n",
    "                nvd_dict[vuln_id]['vuln_avail'] = vuln_avail\n",
    "            elif 'vuln:reference href=' in ln:\n",
    "                vuln_link = re.search('href=.*\\\"', ln).group(0)[6:-1]\n",
    "                # print '    --->%s'%vuln_link\n",
    "                nvd_dict[vuln_id]['vuln_link'] = vuln_link\n",
    "            elif '<vuln:summary>' in ln:\n",
    "                vuln_summ = re.search('>.*<', ln).group(0)[1:-1]\n",
    "                # print '    --->%s'%vuln_summ\n",
    "                nvd_dict[vuln_id]['vuln_summ'] = vuln_summ\n",
    "            elif '<vuln:source>' in ln:\n",
    "                vuln_source = re.search('>.*<', ln).group(0)[1:-1]\n",
    "                # print '    --->%s'%vuln_summ\n",
    "                nvd_dict[vuln_id]['vuln_source'] = vuln_source\n",
    "    return(nvd_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the function we defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the defined function on the NVD 2015 xml file extracted to the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvd = pull_nvd('./nvdcve-2.0-2015.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a timeline for 2015 so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a timeline for 2015 so far by count for both the unweighted instances and the weighted totals given the CVSS score provided. Both the amount of vulnerabilities reported and the evaluated threat they pose on that day is of interest. We compute a running sum for both to see how much of a difference there is day to day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nvd_timeline = [[],[]]\n",
    "for cve in tqdm(nvd.keys()):\n",
    "    if 'vuln_score' in nvd[cve].keys():\n",
    "        weightedme = {'Method':'Weighted', 'Date':nvd[cve]['vuln_datetime'], 'Value':float(nvd[cve]['vuln_score'])}\n",
    "        countme = {'Method':'Count', 'Date':nvd[cve]['vuln_datetime'], 'Value':1}\n",
    "        if nvd[cve]['vuln_datetime'] not in [ x['Date'] for x in nvd_timeline[0] ]:\n",
    "            nvd_timeline[0].append(weightedme)\n",
    "            nvd_timeline[1].append(countme)\n",
    "        else:\n",
    "            for i, day in enumerate(nvd_timeline[0]):\n",
    "                if nvd[cve]['vuln_datetime'] in day['Date'] and day['Method'] == 'Weighted':\n",
    "                    nvd_timeline[0][i]['Value'] += float(nvd[cve]['vuln_score'])\n",
    "                elif nvd[cve]['vuln_datetime'] in day['Date'] and day['Method'] == 'Count':\n",
    "                    nvd_timeline[1][i]['Value'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export our newly structured data to the local working directory as a JSON. This is now an input for our d3.js + dimple.js scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./nvd_timeline.json', 'wb') as f:\n",
    "    json.dump(nvd_timeline, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a count of vulnerabilities per platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are interested in the number of vulnerabilities reported by platform. Does conventional wisdom hold up? Does Apple receive less reported vulnerabilities than Microsoft? Does Linux receive even less? To answer this, we create a new entry for each new platform and do a running sum as we parse through our defined dictionary. We then sort the resulting list and add a header to make loading in javascript faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nvd_plat = []\n",
    "for cve in tqdm(nvd.keys()):\n",
    "    if 'vuln_score' in nvd[cve].keys():\n",
    "        for os in nvd[cve]['vuln_os']:\n",
    "            appendme = [os, 1]\n",
    "            if os not in [ x[0] for x in nvd_plat ]:\n",
    "                nvd_plat.append(appendme)\n",
    "            else:\n",
    "                for i, plat in enumerate(nvd_plat):\n",
    "                    if os == plat[0]:\n",
    "                        nvd_plat[i][1] += 1\n",
    "\n",
    "nvd_plat_sort = [['plat','count']]\n",
    "nvd_plat_sort.extend(sorted(nvd_plat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export our (again) newly structured data this time to a CSV in the local working directory. We show how different file types can be exported from Python. We can then use our exported file as an input for d3.js + dimple.js histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./nvd_plat.csv', 'wb') as csvfile:\n",
    "    writeme = csv.writer(csvfile, delimiter=',')\n",
    "    writeme.writerows(nvd_plat_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network graph with weekly information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to look at what platforms are being attacked for each week. We do this by creating a dictionary of nodes and edges with the additional week number, day number, and threat score loaded. Then given the dictionary, export to JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nvd_json = {'nodes':[],'links':[]}\n",
    "for cve in nvd.keys():\n",
    "    if 'vuln_os' in nvd[cve].keys():\n",
    "        append_vuln = {'name':cve, 'group':nvd[cve]['vuln_date'][2], 'week':nvd[cve]['vuln_date'][1], 'threat':float(nvd[cve]['vuln_score']), 'type':1}\n",
    "        nvd_json['nodes'].append(append_vuln)\n",
    "        for os in nvd[cve]['vuln_os']:\n",
    "            append_plat = {'name':os, 'group':nvd[cve]['vuln_date'][2], 'week':nvd[cve]['vuln_date'][1], 'threat':0, 'type':0}\n",
    "            if append_plat not in nvd_json['nodes']:\n",
    "                nvd_json['nodes'].append(append_plat)\n",
    "            nvd_json['links'].append({'source':cve, 'target':os, 'group':nvd[cve]['vuln_date'][2], 'week':nvd[cve]['vuln_date'][1], 'threat':float(nvd[cve]['vuln_score'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now export to JSON for our d3.js network graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./nvd.json', 'wb') as f:\n",
    "    json.dump(nvd_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build inputs for Gephi (Big Annual Network Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weekly slices are important but we also want to see the whole network of vulnerabilities and platforms for 2015 at once. The number of nodes (platforms and vulnerabilities) and edges are too big for an interactive d3.js visualization. We instead use [Gephi](http://gephi.github.io/). Gephi requires an edge list and (optionally) a node list (if unconnected nodes exist). We create both in this exercise as list arrays with headers filled in. Then populate each row with either the edge or node. Finally, writing both files to csv files in working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nvd_edge = [['source','target','value']]\n",
    "for cve in nvd.keys():\n",
    "    if 'vuln_score' in nvd[cve].keys():\n",
    "        threat = nvd[cve]['vuln_score']\n",
    "    else:\n",
    "        threat = 0.0\n",
    "    if 'vuln_os' in nvd[cve].keys():\n",
    "        cve_type = (nvd[cve]['vuln_vector'], nvd[cve]['vuln_compl'], nvd[cve]['vuln_auth'], nvd[cve]['vuln_confid'], nvd[cve]['vuln_integ'])\n",
    "        for plat in nvd[cve]['vuln_os']:\n",
    "            appendme = [cve_type, plat, threat]\n",
    "            if appendme not in nvd_edge:\n",
    "                nvd_edge.append(appendme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nvd_node = [['id','value','type']]\n",
    "for row in nvd_edge:\n",
    "    append_vuln = [row[0],row[2],0]\n",
    "    append_targ = [row[1],0.0,1]\n",
    "    if append_vuln not in nvd_node:\n",
    "        nvd_node.append(append_vuln)\n",
    "    if append_targ not in nvd_node:\n",
    "        nvd_node.append(append_targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now export them to CSV for Gephi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./nvd_edge.csv', 'wb') as csvfile:\n",
    "    writeme = csv.writer(csvfile, delimiter=',')\n",
    "    writeme.writerows(nvd_edge)\n",
    "\n",
    "with open('./nvd_node.csv', 'wb') as csvfile:\n",
    "    writeme = csv.writer(csvfile, delimiter=',')\n",
    "    writeme.writerows(nvd_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at other years of the NVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our look at NVD in 2015, we want to expand our look to past years. We do this by expanding our original dictionary to go beyond 2015. We load previous years of the NVD with the same function and update the original dictionary we defined. Then count for each week how many vulnerabilities are reported for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nvd.update(pull_nvd('./nvdcve-2.0-2011.xml'))\n",
    "nvd.update(pull_nvd('./nvdcve-2.0-2012.xml'))\n",
    "nvd.update(pull_nvd('./nvdcve-2.0-2013.xml'))\n",
    "nvd.update(pull_nvd('./nvdcve-2.0-2014.xml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with our newly beefed up dictionary that includes 2011 to 2015 datasets from NVD, we start processing to create time series for all five years. To do this we look at iso week number for any given year and conducting a running sum of the number of vulnerabilities reported. Commented out is an alternative look at the combined CVSS score for that week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nvd_timeline_five = []\n",
    "for cve in tqdm(nvd.keys()):\n",
    "    if 'vuln_score' in nvd[cve].keys():\n",
    "        appendme = [nvd[cve]['vuln_date'][1],0,0,0,0,0]\n",
    "        if nvd[cve]['vuln_date'][0] == 2015:\n",
    "            colnum = 1\n",
    "        elif nvd[cve]['vuln_date'][0] == 2014:\n",
    "            colnum = 2\n",
    "        elif nvd[cve]['vuln_date'][0] == 2013:\n",
    "            colnum = 3\n",
    "        elif nvd[cve]['vuln_date'][0] == 2012:\n",
    "            colnum = 4\n",
    "        elif nvd[cve]['vuln_date'][0] == 2011:\n",
    "            colnum = 5\n",
    "        if nvd[cve]['vuln_date'][1] not in [ x[0] for x in nvd_timeline_five ]:\n",
    "            # appendme[colnum] += float(nvd[cve]['vuln_score'])\n",
    "            appendme[colnum] += 1\n",
    "            nvd_timeline_five.append(appendme)\n",
    "        else:        \n",
    "            for i, day in enumerate(nvd_timeline_five):\n",
    "                if nvd[cve]['vuln_date'][1] == day[0]:\n",
    "                    # nvd_timeline_five[i][colnum] += float(nvd[cve]['vuln_score'])\n",
    "                    nvd_timeline_five[i][colnum] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before we want to sort the resulting structured data and attach a header to it. This reduces processing on the javascript end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nvd_timeline_five_sort = [['week','2015','2014','2013','2012','2011']]\n",
    "nvd_timeline_five_sort.extend(sorted(nvd_timeline_five))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can export it to a CSV for our dygraph.js plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./nvd_timeline_five.csv', 'wb') as csvfile:\n",
    "    writeme = csv.writer(csvfile, delimiter=',')\n",
    "    writeme.writerows(nvd_timeline_five_sort)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
